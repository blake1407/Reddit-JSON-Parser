{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wl/yn_f16552qvcz999s7pwv5sm0000gn/T/ipykernel_68958/1301464876.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "import os\n",
    "   \n",
    "import pandas as pd \n",
    "import spacy \n",
    "# spacy.cli.download(\"pt_core_news_sm\")\n",
    "# spacy.cli.download(\"es_core_news_sm\")\n",
    "\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "from dataclasses import dataclass\n",
    "\n",
    "#Sentence Tokenization using sent_tokenize\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "#Detect language using detect_langs\n",
    "import langdetect\n",
    "from langdetect import detect_langs\n",
    "\n",
    "#Detect language using Lingua\n",
    "from lingua import Language, LanguageDetectorBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments yielded for the corpus (that are not urls or deleted): 91.\n",
      "Number of removed/deleted comments (has been filetered from corpus): 5.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "First, I'm gonna get the entire corpus from the \"Reddit Post Parsed\" folder.\n",
    "\"\"\"\n",
    "all_post_titles = []\n",
    "expected_no_comments = 0\n",
    "corpus = \"\"\n",
    "comment_urls = []\n",
    "all_links = []\n",
    "\n",
    "with open('log.csv', mode = 'r') as file:\n",
    "    link_column = []\n",
    "    title_column = []\n",
    "    comments_column = []\n",
    "    all_no_comments = []\n",
    "    csvFile = csv.reader(file)\n",
    "    for line in csvFile:\n",
    "        title_column.append(line[2])\n",
    "        all_post_titles = title_column[1:]\n",
    "        comments_column.append(line[9])\n",
    "        all_no_comments = comments_column[1:]\n",
    "        link_column.append(line[3])\n",
    "        all_links = link_column[1:]\n",
    "    for number in all_no_comments:\n",
    "        expected_no_comments += int(number)\n",
    "\n",
    "#loop to open all post titles in create one big corpus of all comments\n",
    "def create_corpus(titles: list) -> str:\n",
    "    \"\"\"\n",
    "    This function takes in a list of posts titles in the \n",
    "    folder \"Reddit Post Parsed\" and loops through each \n",
    "    csv file to filter for proper comments, that are not urls\n",
    "    and deleted to return the corpus.\n",
    "\n",
    "    Comments that are just links will be \n",
    "    appended to the list \"comment_urls\"!\n",
    "    \"\"\"\n",
    "    global corpus\n",
    "    global comment_urls\n",
    "\n",
    "    count_proper_comments = 0\n",
    "    no_deleted_comments = 0\n",
    "    empty = \"\"\n",
    "    list_of_comments = []\n",
    "    \n",
    "    base_folder = \"Update posts files\"\n",
    "    for title in titles:\n",
    "        title_csv = os.path.join(base_folder, title + \"'s post.csv\")\n",
    "        if not os.path.isfile(title_csv):\n",
    "            print(f\"File '{title_csv}' not found.\")\n",
    "            continue\n",
    "    \n",
    "        with open(title_csv, mode='r', encoding='utf-8') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "            for row in csv_reader:\n",
    "                list_of_comments.append(empty.join(row[9:]))\n",
    "\n",
    "    for comment in list_of_comments:\n",
    "        if comment.strip() != \"Body\":\n",
    "            if comment.strip() == '\"deleted\"' or comment.strip() == '\"removed\"':\n",
    "                no_deleted_comments +=1\n",
    "                comment = \"\"\n",
    "            if comment.strip().startswith('\"https:'):\n",
    "                comment_urls.append(comment.replace('\"', \"\").strip())\n",
    "            else:\n",
    "                count_proper_comments += 1 \n",
    "                corpus = corpus + \" \" + comment.replace(\"**\", \"\").replace(\"#\", \"\").strip()[1:-1] \n",
    "    print(f'Number of comments yielded for the corpus (that are not urls or deleted): {count_proper_comments}.') \n",
    "    print(f'Number of removed/deleted comments (has been filetered from corpus): {no_deleted_comments}.\\n')                  \n",
    "                \n",
    "create_corpus(all_post_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:17: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\p'\n",
      "/var/folders/wl/yn_f16552qvcz999s7pwv5sm0000gn/T/ipykernel_68958/3812967711.py:17: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  tokenizer = nltk.data.load('tokenizers\\punkt\\english.pickle')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of total sentence tokens: 171.\n",
      "Amount of total word token: 1550.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class entities:\n",
    "    name: str\n",
    "    label: str\n",
    "\n",
    "#strip out the pronouns, conjunctions, etc.!\n",
    "f = open('stop words.txt', 'r')\n",
    "stopwords = f.read()\n",
    "stopwords = stopwords.split('\\n')\n",
    "\n",
    "# Load the spaCy English & Portuguese models\n",
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "pt_nlp = spacy.load('pt_core_news_sm')\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "\n",
    "#separate into tokenized sentences\n",
    "tokenizer = nltk.data.load('tokenizers\\punkt\\english.pickle')\n",
    "sentences_token = tokenizer.tokenize(corpus)\n",
    "sentences = []\n",
    "for sentence in sentences_token:\n",
    "    if sentence.strip() not in stopwords:\n",
    "        sentences.append(sentence)\n",
    "\n",
    "#separate corpus in words\n",
    "words_token = word_tokenize(corpus)\n",
    "words = []\n",
    "#remove any conjunctions, articles, particles, etc.\n",
    "for word in words_token:\n",
    "    if word.lower().strip() not in stopwords:\n",
    "        words.append(word)\n",
    "\n",
    "def checkW(x: int):\n",
    "    return (x/len(words))*100\n",
    "\n",
    "def checkS(x: int):\n",
    "    return (x/len(sentences))*100\n",
    "\n",
    "f.close()\n",
    "\n",
    "print(f'Amount of total sentence tokens: {len(sentences)}.')\n",
    "print(f'Amount of total word token: {len(words)}.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Langdetect sentence by sentence\n",
      "English sentences: 156 - 91.23%.\n",
      "Portuguese sentences: 1 - 0.58%.\n",
      "Spanish sentences: 0 - 0.00%.\n",
      "Mixed sentences: 14 - 8.19%.\n",
      "Discarded: 0 - 0.00%.\n",
      "Amount detected from total: 100.00%.\n",
      "\n",
      "2. Langdetect word by word - just bad - all have benchmark over 0.8\n",
      "Amount of language detected: 31.\n",
      "Language detected: ['en', 'it', 'hu', 'id', 'es', 'ca', 'cy', 'fi', 'fr', 'sv', 'ro', 'vi', 'no', 'hr', 'nl', 'af', 'da', 'de', 'so', 'pt', 'tl', 'sw', 'sk', 'sq', 'pl', 'lv', 'et', 'sl', 'tr', 'lt', 'cs'].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# METHOD 1: Langdetect each sentence in the corpus\n",
    "\n",
    "#Sentences with more than one languages detected\n",
    "mixed_s = []\n",
    "#Sentences with only one language detected\n",
    "en_s = []\n",
    "es_s = []\n",
    "pt_s = []\n",
    "discarded = 0\n",
    "for sentence in sentences:\n",
    "    #benchmark is with a language probablity over 0.5\n",
    "    try:\n",
    "        score = detect_langs(sentence)\n",
    "        if len(score) > 1:\n",
    "            mixed_s.append(sentence)\n",
    "        elif len(score) == 1:\n",
    "            for s in score:\n",
    "                if s.lang == 'en' and s.prob >= 0.8:\n",
    "                    en_s.append(sentence)\n",
    "                elif s.lang == 'es' and s.prob >= 0.8:\n",
    "                    es_s.append(sentence)\n",
    "                elif s.lang == 'pt' and s.prob >= 0.8:\n",
    "                    pt_s.append(sentence)\n",
    "                else: \n",
    "                    mixed_s.append(sentence)\n",
    "    except:\n",
    "        #discard \".\" or numbers\n",
    "        # print(\"This throws an error: \" + sentence)\n",
    "        discarded +=1\n",
    "        continue\n",
    "\n",
    "print(\"1. Langdetect sentence by sentence\")\n",
    "print(f'English sentences: {len(en_s)} - {checkS(len(en_s)):.2f}%.')\n",
    "print(f'Portuguese sentences: {len(pt_s)} - {checkS(len(pt_s)):.2f}%.')\n",
    "print(f'Spanish sentences: {len(es_s)} - {checkS(len(es_s)):.2f}%.')\n",
    "print(f'Mixed sentences: {len(mixed_s)} - {checkS(len(mixed_s)):.2f}%.')\n",
    "print(f'Discarded: {discarded} - {checkS(discarded):.2f}%.')\n",
    "print(f'Amount detected from total: {checkS(len(en_s) + len(pt_s) + len(es_s)+ len(mixed_s)):.2f}%.\\n')\n",
    "\n",
    "\n",
    "# METHOD 2: Langdetect word by word\n",
    "language_detected = []\n",
    "for word in words:\n",
    "    #benchmark is with a language probablity over 0.5\n",
    "    try:\n",
    "        w_score = detect_langs(word)\n",
    "        for s in w_score:\n",
    "            if s.lang not in language_detected and s.prob > 0.8:\n",
    "                language_detected.append(s.lang)\n",
    "    except:\n",
    "        #discard punctuations or numbers\n",
    "        # print(\"This throws an error: \" + word)\n",
    "        continue \n",
    "print(\"2. Langdetect word by word - just bad - all have benchmark over 0.8\")\n",
    "print(f'Amount of language detected: {len(language_detected)}.')\n",
    "print(f'Language detected: {language_detected}.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Lingua sentence by sentence\n",
      "English sentences: 156  - 91.23%.\n",
      "Portuguese sentences: 0 - 0.00%.\n",
      "Spanish sentences: 0 - 0.00%.\n",
      "Mixed sentences: 15 - 8.77%.\n",
      "Discarded: 0 - 0.00%.\n",
      "Amount detected from total: 100.00%.\n",
      "\n",
      "4. Lingua word by word\n",
      "English words: 1162 - 74.97%.\n",
      "Portuguese words: 46 - 2.97%.\n",
      "Spanish words: 28 - 1.81%.\n",
      "Mixed words: 314 - 20.26%.\n",
      "Discarded: 0 - 0.00%.\n",
      "Amount detected from total: 100.00%.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# METHOD 3: Lingua sentence by sentence\n",
    "\n",
    "#import English, Portuguese, Spanish detector\n",
    "languages = [Language.ENGLISH, Language.PORTUGUESE, Language.SPANISH]\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "\n",
    "mixed_sentences = []\n",
    "english_sentences = []\n",
    "portuguese_sentences = []\n",
    "spanish_sentence = []\n",
    "\n",
    "discarded_l = 0\n",
    "for sentence in sentences:\n",
    "    try:\n",
    "        en_l = detector.compute_language_confidence(sentence, Language.ENGLISH)\n",
    "        pt_l = detector.compute_language_confidence(sentence, Language.PORTUGUESE)\n",
    "        es_l = detector.compute_language_confidence(sentence, Language.SPANISH)\n",
    "        if en_l > 0.8:\n",
    "            english_sentences.append(sentence)\n",
    "        elif pt_l > 0.8:\n",
    "            portuguese_sentences.append(sentence)\n",
    "        elif es_l > 0.8:\n",
    "            spanish_sentence.append(sentence)\n",
    "        else:\n",
    "            mixed_sentences.append(sentence)\n",
    "    except: \n",
    "        #discard \".\" or numbers\n",
    "        # print(\"This throws an error: \" + sentence)\n",
    "        discarded_l +=1\n",
    "        continue\n",
    "\n",
    "print(\"3. Lingua sentence by sentence\")\n",
    "print(f'English sentences: {len(english_sentences)}  - {checkS(len(english_sentences)):.2f}%.')\n",
    "print(f'Portuguese sentences: {len(portuguese_sentences)} - {checkS(len(portuguese_sentences)):.2f}%.')\n",
    "print(f'Spanish sentences: {len(spanish_sentence)} - {checkS(len(spanish_sentence)):.2f}%.')\n",
    "print(f'Mixed sentences: {len(mixed_sentences)} - {checkS(len(mixed_sentences)):.2f}%.')\n",
    "print(f'Discarded: {discarded_l} - {checkS(discarded_l):.2f}%.')\n",
    "print(f'Amount detected from total: {checkS(len(english_sentences) + len(portuguese_sentences) + len(spanish_sentence)+ len(mixed_sentences)):.2f}%.\\n')\n",
    "\n",
    "\n",
    "\n",
    "# METHOD 4: Lingua word by word\n",
    "\n",
    "en_w = []\n",
    "pt_w = []\n",
    "es_w = []\n",
    "mixed_w = []\n",
    "\n",
    "discard_w = 0\n",
    "for word in words:\n",
    "    try:\n",
    "        en_l = detector.compute_language_confidence(word, Language.ENGLISH)\n",
    "        pt_l = detector.compute_language_confidence(word, Language.PORTUGUESE)\n",
    "        es_l = detector.compute_language_confidence(word, Language.SPANISH)\n",
    "        if en_l > 0.5:\n",
    "            en_w.append(word)\n",
    "        elif pt_l > 0.5:\n",
    "            pt_w.append(word)\n",
    "        elif es_l > 0.5:\n",
    "            es_w.append(word)\n",
    "        else:\n",
    "            mixed_w.append(word)\n",
    "    except: \n",
    "        #discard \".\" or numbers\n",
    "        # print(\"This throws an error: \" + sentence)\n",
    "        discard_w +=1\n",
    "        continue\n",
    "            \n",
    "print(\"4. Lingua word by word\")\n",
    "print(f'English words: {len(en_w)} - {checkW(len(en_w)):.2f}%.')\n",
    "print(f'Portuguese words: {len(pt_w)} - {checkW(len(pt_w)):.2f}%.')\n",
    "print(f'Spanish words: {len(es_w)} - {checkW(len(es_w)):.2f}%.')\n",
    "print(f'Mixed words: {len(mixed_w)} - {checkW(len(mixed_w)):.2f}%.')\n",
    "print(f'Discarded: {discard_w} - {checkW(discard_w):.2f}%.')\n",
    "print(f'Amount detected from total: {checkW(len(en_w) + len(pt_w) + len(es_w)+ len(mixed_w)):.2f}%.\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. LangID sentence by sentence\n",
      "English sentences: 164  - 95.91%.\n",
      "Portuguese sentences: 0 - 0.00%.\n",
      "Spanish sentences: 0 - 0.00%.\n",
      "Mixed sentences: 7 - 4.09%.\n",
      "Discarded: 0 - 0.00%.\n",
      "Amount detected from total: 100.00%.\n",
      "\n",
      "6. LangID word by word\n",
      "English words: 143 - 9.23%.\n",
      "Portuguese words: 0 - 0.00%.\n",
      "Spanish words: 1 - 0.06%.\n",
      "Mixed words: 1406 - 90.71%.\n",
      "Discarded: 0 - 0.00%.\n",
      "Amount detected from total: 100.00%.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# METHOD 5: LangID sentence by sentence\n",
    "\n",
    "import langid\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "\n",
    "mixed_sentences = []\n",
    "english_sentences = []\n",
    "portuguese_sentences = []\n",
    "spanish_sentence = []\n",
    "\n",
    "discarded_l = 0\n",
    "for sentence in sentences:\n",
    "    try:\n",
    "        #constrain the language set\n",
    "        langid.set_languages(['en','pt','es'])\n",
    "        lang, conf = identifier.classify(sentence)\n",
    "        if conf > 0.5 and lang == 'en':\n",
    "            english_sentences.append(sentence)\n",
    "        elif conf > 0.5 and lang == 'pt':\n",
    "            portuguese_sentences.append(sentence)\n",
    "        elif conf > 0.5 and lang == 'es':\n",
    "            spanish_sentence.append(sentence)\n",
    "        else:\n",
    "            mixed_sentences.append(sentence)\n",
    "    except: \n",
    "        #discard \".\" or numbers\n",
    "        # print(\"This throws an error: \" + sentence)\n",
    "        discarded_l +=1\n",
    "        continue\n",
    "print(\"5. LangID sentence by sentence\")\n",
    "print(f'English sentences: {len(english_sentences)}  - {checkS(len(english_sentences)):.2f}%.')\n",
    "print(f'Portuguese sentences: {len(portuguese_sentences)} - {checkS(len(portuguese_sentences)):.2f}%.')\n",
    "print(f'Spanish sentences: {len(spanish_sentence)} - {checkS(len(spanish_sentence)):.2f}%.')\n",
    "print(f'Mixed sentences: {len(mixed_sentences)} - {checkS(len(mixed_sentences)):.2f}%.')\n",
    "print(f'Discarded: {discarded_l} - {checkS(discarded_l):.2f}%.')\n",
    "print(f'Amount detected from total: {checkS(len(english_sentences) + len(portuguese_sentences) + len(spanish_sentence)+ len(mixed_sentences)):.2f}%.\\n')\n",
    "\n",
    "\n",
    "# METHOD 6: LangID word by word\n",
    "\n",
    "en_w = []\n",
    "pt_w = []\n",
    "es_w = []\n",
    "mixed_w = []\n",
    "discard_w = 0\n",
    "for word in words:\n",
    "    try:\n",
    "        #constrain the language set\n",
    "        langid.set_languages(['en','pt','es'])\n",
    "        lang, conf = identifier.classify(word)\n",
    "        if conf > 0.5 and lang == 'en':\n",
    "            en_w.append(word)\n",
    "        elif conf > 0.8 and lang == 'pt':\n",
    "            pt_w.append(word)\n",
    "        elif conf > 0.8 and lang == 'es':\n",
    "            es_w.append(word)\n",
    "        else:\n",
    "            mixed_w.append(word)\n",
    "    except: \n",
    "        #discard \".\" or numbers\n",
    "        # print(\"This throws an error: \" + sentence)\n",
    "        discard_w +=1\n",
    "        continue\n",
    "            \n",
    "print(\"6. LangID word by word\")\n",
    "print(f'English words: {len(en_w)} - {checkW(len(en_w)):.2f}%.')\n",
    "print(f'Portuguese words: {len(pt_w)} - {checkW(len(pt_w)):.2f}%.')\n",
    "print(f'Spanish words: {len(es_w)} - {checkW(len(es_w)):.2f}%.')\n",
    "print(f'Mixed words: {len(mixed_w)} - {checkW(len(mixed_w)):.2f}%.')\n",
    "print(f'Discarded: {discard_w} - {checkW(discard_w):.2f}%.')\n",
    "print(f'Amount detected from total: {checkW(len(en_w) + len(pt_w) + len(es_w)+ len(mixed_w)):.2f}%.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-05-08 23:08:44 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 44.1MB/s]                    \n",
      "2024-05-08 23:08:45 INFO: Downloaded file to /Users/blakey/stanza_resources/resources.json\n",
      "2024-05-08 23:08:45 INFO: Loading these models for language: multilingual ():\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| langid    | ud      |\n",
      "=======================\n",
      "\n",
      "2024-05-08 23:08:45 INFO: Using device: cpu\n",
      "2024-05-08 23:08:45 INFO: Loading: langid\n",
      "2024-05-08 23:08:45 INFO: Done loading processors!\n",
      "2024-05-08 23:08:46 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. Stanza sentence by sentence\n",
      "English sentences: 170  - 99.42%.\n",
      "Portuguese sentences: 0 - 0.00%.\n",
      "Spanish sentences: 1 - 0.58%.\n",
      "Mixed sentences: 0 - 0.00%.\n",
      "Amount detected from total: 100.00%.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 19.0MB/s]                    \n",
      "2024-05-08 23:08:46 INFO: Downloaded file to /Users/blakey/stanza_resources/resources.json\n",
      "2024-05-08 23:08:46 INFO: Loading these models for language: multilingual ():\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| langid    | ud      |\n",
      "=======================\n",
      "\n",
      "2024-05-08 23:08:46 INFO: Using device: cpu\n",
      "2024-05-08 23:08:46 INFO: Loading: langid\n",
      "2024-05-08 23:08:46 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8. Stanza word by word\n",
      "English words: 1336 - 86.19%.\n",
      "Portuguese words: 146 - 9.42%.\n",
      "Spanish words: 68 - 4.39%.\n",
      "Mixed words: 0 - 0.00%.\n",
      "Amount detected from total: 100.00%.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# METHOD 7: Stanza sentence by sentence\n",
    "\n",
    "import stanza\n",
    "# stanza.download(lang=\"multilingual\")\n",
    "# stanza.download(lang=\"en\")\n",
    "# stanza.download(lang=\"es\")\n",
    "# stanza.download(lang=\"pt\")\n",
    "from stanza.models.common.doc import Document\n",
    "from stanza.pipeline.core import Pipeline\n",
    "\n",
    "mixed_sentences = []\n",
    "english_sentences = []\n",
    "portuguese_sentences = []\n",
    "spanish_sentence = []\n",
    "\n",
    "nlp = Pipeline(lang=\"multilingual\", processors=\"langid\",langid_lang_subset=[\"en\",\"es\",\"pt\"], langid_clean_text=True)\n",
    "docs = sentences\n",
    "docs = [Document([], text=text) for text in docs]\n",
    "nlp(docs)\n",
    "for doc in docs:\n",
    "    if doc.lang == 'en':\n",
    "        english_sentences.append(doc)\n",
    "    elif doc.lang == 'es':\n",
    "        spanish_sentence.append(doc)\n",
    "    elif doc.lang == 'pt':\n",
    "        portuguese_sentences.append(doc)\n",
    "    else:\n",
    "        mixed_sentences.append(doc)\n",
    "\n",
    "print(\"7. Stanza sentence by sentence\")\n",
    "print(f'English sentences: {len(english_sentences)}  - {checkS(len(english_sentences)):.2f}%.')\n",
    "print(f'Portuguese sentences: {len(portuguese_sentences)} - {checkS(len(portuguese_sentences)):.2f}%.')\n",
    "print(f'Spanish sentences: {len(spanish_sentence)} - {checkS(len(spanish_sentence)):.2f}%.')\n",
    "print(f'Mixed sentences: {len(mixed_sentences)} - {checkS(len(mixed_sentences)):.2f}%.')\n",
    "print(f'Amount detected from total: {checkS(len(english_sentences) + len(portuguese_sentences) + len(spanish_sentence)+ len(mixed_sentences)):.2f}%.\\n') \n",
    "\n",
    "# METHOD 8: Stanza word by word\n",
    "en_w = []\n",
    "pt_w = []\n",
    "es_w = []\n",
    "mixed_w = []\n",
    "discard_w = 0\n",
    "\n",
    "nlp = Pipeline(lang=\"multilingual\", processors=\"langid\",langid_lang_subset=[\"en\",\"es\",\"pt\"], langid_clean_text=True)\n",
    "docs = words\n",
    "docs = [Document([], text=text) for text in docs]\n",
    "nlp(docs)\n",
    "\n",
    "for doc in docs:\n",
    "    if doc.lang == 'en':\n",
    "        en_w.append(doc)\n",
    "    elif doc.lang == 'es':\n",
    "        es_w.append(doc)\n",
    "    elif doc.lang == 'pt':\n",
    "        pt_w.append(doc)\n",
    "    else:\n",
    "        mixed_w.append(doc)\n",
    "            \n",
    "print(\"8. Stanza word by word\")\n",
    "print(f'English words: {len(en_w)} - {checkW(len(en_w)):.2f}%.')\n",
    "print(f'Portuguese words: {len(pt_w)} - {checkW(len(pt_w)):.2f}%.')\n",
    "print(f'Spanish words: {len(es_w)} - {checkW(len(es_w)):.2f}%.')\n",
    "print(f'Mixed words: {len(mixed_w)} - {checkW(len(mixed_w)):.2f}%.')\n",
    "print(f'Amount detected from total: {checkW(len(en_w) + len(pt_w) + len(es_w)+ len(mixed_w)):.2f}%.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 9: xlm-roberta-base-language-detection sentence by sentence\n",
    "\n",
    "# METHOD 10: xlm-roberta-base-language-detection word by word\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"papluca/xlm-roberta-base-language-detection\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"papluca/xlm-roberta-base-language-detection\")\n",
    "    return tokenizer, model\n",
    "\n",
    "def predict_language(text, tokenizer, model):\n",
    "    # Encode the text using the tokenizer\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    \n",
    "    # Predict the language using the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the predicted language ID (the highest probability)\n",
    "    logits = outputs.logits\n",
    "    predicted_id = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    # Convert the language ID to language code\n",
    "    labels = model.config.id2label\n",
    "    predicted_language = labels[predicted_id]\n",
    "    \n",
    "    return predicted_language\n",
    "#sentences\n",
    "mixed_sentences = []\n",
    "english_sentences = []\n",
    "portuguese_sentences = []\n",
    "spanish_sentence = []\n",
    "\n",
    "#words\n",
    "en_w = []\n",
    "pt_w = []\n",
    "es_w = []\n",
    "mixed_w = []\n",
    "\n",
    "def main():\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer, model = load_model_and_tokenizer()\n",
    "    \n",
    "    # Example text to detect the language\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Predict the language\n",
    "        language = predict_language(sentence, tokenizer, model)\n",
    "        if language == 'en':\n",
    "            english_sentences.append(sentence)\n",
    "        elif language == 'es':\n",
    "            spanish_sentence.append(sentence)\n",
    "        elif language == 'pt':\n",
    "            portuguese_sentences.append(sentence)\n",
    "        else:\n",
    "            mixed_sentences.append(sentence)\n",
    "\n",
    "    for word in words:\n",
    "        # Predict the language\n",
    "        language = predict_language(word, tokenizer, model)\n",
    "        if language == 'en':\n",
    "            en_w.append(word)\n",
    "        elif language == 'es':\n",
    "            es_w.append(word)\n",
    "        elif language == 'pt':\n",
    "            pt_w.append(word)\n",
    "        else:\n",
    "            mixed_w.append(word)\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "print(\"9. xlm-roberta-base-language-detection sentence by sentence\")\n",
    "print(f'English sentences: {len(english_sentences)}  - {checkS(len(english_sentences)):.2f}%.')\n",
    "print(f'Portuguese sentences: {len(portuguese_sentences)} - {checkS(len(portuguese_sentences)):.2f}%.')\n",
    "print(f'Spanish sentences: {len(spanish_sentence)} - {checkS(len(spanish_sentence)):.2f}%.')\n",
    "print(f'Mixed sentences: {len(mixed_sentences)} - {checkS(len(mixed_sentences)):.2f}%.')\n",
    "print(f'Amount detected from total: {checkS(len(english_sentences) + len(portuguese_sentences) + len(spanish_sentence)+ len(mixed_sentences)):.2f}%.\\n') \n",
    "\n",
    "print(\"10. xlm-roberta-base-language-detection word by word\")\n",
    "print(f'English words: {len(en_w)} - {checkW(len(en_w)):.2f}%.')\n",
    "print(f'Portuguese words: {len(pt_w)} - {checkW(len(pt_w)):.2f}%.')\n",
    "print(f'Spanish words: {len(es_w)} - {checkW(len(es_w)):.2f}%.')\n",
    "print(f'Mixed words: {len(mixed_w)} - {checkW(len(mixed_w)):.2f}%.')\n",
    "print(f'Amount detected from total: {checkW(len(en_w) + len(pt_w) + len(es_w)+ len(mixed_w)):.2f}%.\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
